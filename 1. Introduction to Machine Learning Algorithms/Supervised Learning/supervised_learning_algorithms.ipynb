# -*- coding: utf-8 -*-
"""Supervised_Learning_algorithms.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VpXEprhtPk93gt4V2KEBnadFU-lj-QhT
"""

# Importing the libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the datasets

datasets = pd.read_csv('adds.csv')
X = datasets.iloc[:, [2,3]].values
Y = datasets.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set

from sklearn.model_selection import train_test_split
X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_Train = sc_X.fit_transform(X_Train)
X_Test = sc_X.transform(X_Test)

"""# Decision Tree Classifier"""

# Fitting the classifier into the Training set
from sklearn.tree import DecisionTreeClassifier
classifier1 = DecisionTreeClassifier(criterion = 'entropy', random_state = 13)
classifier1.fit(X_Train,Y_Train)

Y_Pred = classifier1.predict(X_Test)
print(Y_Pred)
print(" \n The accuracy of the model is=", classifier1.score(X_Test,Y_Test)*100,"%")

# Making the Confusion Matrix 

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_Test, Y_Pred)
cm

"""# Random Forest"""

# Fitting the classifier into the Training set
from sklearn.ensemble import RandomForestClassifier
classifier2 = RandomForestClassifier(max_depth=7, random_state=13)
classifier2.fit(X_Train,Y_Train)

Y_Pred = classifier2.predict(X_Test)
print(Y_Pred)
print(" \n The accuracy of the model is=", classifier2.score(X_Test,Y_Test)*100,"%")

# Making the Confusion Matrix 

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_Test, Y_Pred)
cm

"""# Naive Bayes"""

# Fitting the classifier into the Training set
from sklearn.naive_bayes import GaussianNB
classifier3 = GaussianNB()
classifier3.fit(X_Train,Y_Train)

Y_Pred = classifier3.predict(X_Test)
print(Y_Pred)
print(" \n The accuracy of the model is=", classifier3.score(X_Test,Y_Test)*100,"%")

# Making the Confusion Matrix 

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_Test, Y_Pred)
cm

"""# Support Vector Machine"""

# Fitting the classifier into the Training set
from sklearn import svm
classifier4 =svm.SVC()
classifier4.fit(X_Train,Y_Train)

Y_Pred = classifier4.predict(X_Test)
print(Y_Pred)
print(" \n The accuracy of the model is=", classifier4.score(X_Test,Y_Test)*100,"%")

# Making the Confusion Matrix 

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_Test, Y_Pred)
cm

"""# Hyperparameter Tuning of Decision Tree Classifier"""

# Import libraries
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn import decomposition
from sklearn.preprocessing import StandardScaler

# # Create an scaler object
sc = StandardScaler()

# Create a pca object
pca = decomposition.PCA()

pipe = Pipeline(steps=[('sc', sc),
                           ('pca', pca),
                           ('decisiontree', classifier1)])

n_components = list(range(1,X.shape[1]+1,1))
# parameters of decision tree classifier
criterion = ['gini', 'entropy']
max_depth = [4,6,8,12]

parameters = dict(pca__n_components=n_components,
                      decisiontree__criterion=criterion,
                      decisiontree__max_depth=max_depth)

clf = GridSearchCV(pipe, parameters,verbose=1)

clf.fit(X_Train, Y_Train)

print('Best Criterion:', clf.best_estimator_.get_params()['decisiontree__criterion'])
print('Best max_depth:', clf.best_estimator_.get_params()['decisiontree__max_depth'])
print('Best Number Of Components:', clf.best_estimator_.get_params()['pca__n_components'])
print(); print(clf.best_estimator_.get_params()['decisiontree'])

optimized_decision_tree=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',
                       max_depth=4, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=13, splitter='best')

optimized_decision_tree.fit(X_Train,Y_Train)

"""# Performance Change Evaluation
### After Optimization Score = 95%
### Before Optimization Score = 90%
"""

optimized_decision_tree.score(X_Test,Y_Test)

"""##### Similarly Hyperparameter Tuning Can be applied for all other classifiers to take them to their best performance"""